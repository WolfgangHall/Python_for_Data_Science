The most common metrics are accuracy, precision, recall, F1 measure,
and ROC AUC score. All of these measures depend on the concepts of true positives,
true negatives, false positives, and false negatives. Positive and negative refer to the
classes. True and false denote whether the predicted class is the same as the true class


A confusion matrix, or contingency table, can be used to
visualize true and false positives and negative


 While accuracy measures the overall correctness of the classifier, it
does not distinguish between false positive errors and false negative errors. Some
applications may be more sensitive to false negatives than false positives, or vice
versa.



Precision and Recall
precision is the
fraction of positive predictions that are correct

Sometimes called sensitivity in medical domains, recall is the fraction of the truly
positive instances that the classifier recognizes.

Individually, precision and recall are seldom informative; they are both incomplete
views of a classifier's performance. Both precision and recall can fail to distinguish
classifiers that perform well from certain types of classifiers that perform poorly



The F1 measure is the harmonic mean, or weighted average, of the precision and
recall scores. Also called the f-measure or the f-score


The F1 measure penalizes classifiers with imbalanced precision and recall scores,
like the trivial classifier that always predicts the positive class


A Receiver Operating Characteristic, or ROC curve, visualizes a classifier's
performance


ROC
curves plot the classifier's recall against its fall-out. Fall-out, or the false positive
rate, is the number of false positives divided by the total number of negatives
